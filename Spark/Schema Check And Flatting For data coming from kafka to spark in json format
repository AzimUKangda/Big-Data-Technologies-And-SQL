Probleam Statements:

1) if there is json data getting pushed to kafka from sensor. after consuming it in spark will get complete json in one column called 'value'.
In order to porform analysis will have to flatten the json. in order to flatten json will need schema.

Suppose Schema is provided in schema registory as string in json format.

How can we convert value column json to spark df using provided schema.

Answer:

val schemaProperties: Map[String,String] = Map("topic1" -> """{"id":"int","name":"string","department":"string"}""")

def getSchema(topic:String): StructType ={
  var dataSchema: StructType = null
  try{
    val jsonString = schemaProperties(topic)
    dataSchema = spark.read.json(Seq(jsonString).toDS).schema
  }catch{
   case ex: Exception => ex.printStackTrace()
   }
   dataSchema
}

def getSchemaDF(topic: String): DataFrame = {
  val schemaDF: DataFrame = null
  try{
    val jsonString = schemaProperties(topic)
    schemaDF = spark.read.json(Seq(jsonString).toDS)
  }catch{
   case ex: Exception => ex.printStackTrace()
   }
   schemaDF
}

def flattenSchema(schema: StructType, delimiter: String = ".", prefix: String = null): Array[Column] = {
   schema.fields.flatMap(structField => {
   val codeColName = if (prefix == null) structField.name
   else prefix + "." + structField.name
   val colName = if(prefix == null) structField.name
   else prefix + delimiter + structField.name
   
   structField.dataType match {
    case st: StructType => flattenSchema(schmea = st, delimiter = delimiter, prefix = colName)
    case _ => Array(col(codeColName).alias(colName))
   }
   
 })
}

sample df :  val df = Seq(("topic1","""{"id":1,"name":"azim","department":{"id":20,"name":"DBA"}}""")).toDF("topic","value")
