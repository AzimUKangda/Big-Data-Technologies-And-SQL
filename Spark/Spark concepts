What is Spark session?
-> Spark session is data structure where driver maintains all information including all executer's location and it's status. spark application begins by creating spark session.
first thing in spark 2.x application.

Steps for Driver and Executers Creation in Client Mode:

1) Main class will launch the Driver at client machine. Once Drivercreated, it will creates the spark session. as soon as driver creates spark session request goes to yarn resource manager to create yarn application.
2) yarn RM creates Application Master. For client mode AM act as executer's launcher. AM reach outs to RM and request for further containers.
3) RM will launch the containers and AM will launch the executers in each containers.
4) After all above steps executers will directly communicates to driver.

Steps For Driver and Executers creation in CLuster Mode:

1) Application Will be submitted to cluster using spark-submit utility.
2) spark-submit utility will send yarn application request to yarn RM.
3) RM will start AM(AM Container). And AM will starts driver will start in AM Container.(this is where difference between client and cluster Mode)
.
4) Once driver started, driver will reach out to RM For more container request to launch Executers.
5) RM will allocates new Containers.
6) Driver starts executers in each containers.

Spark DataFrames:

There is no such class called DataFrame in spark documentation. DataFrame is type defined as DataSet[Row].

There are two types of transofrmations

1) Typed Transformation -> alwayes returns dataset 
2) Untyped Transformations

When you executes any action of dataframes, spark will do to kind of optimiztion

1) pipelining -> which will see and combine certain operation if possible
2) Predicate Pushdown -> Which will do filter operation fist if any.

So optimized logical plan will go to spark engine(comipler) which will generate physical paln which is nothing but series of rdds.


** spark data frames always uses spark data type irrespective of language used.

** Data Frame Methods to create views.

There are two type of view possible using DataFrame Methods

1) Global Tmp View --> Visible at application level. will be available across the session.
2) Local Tmp View --> visible only at session level.

both leaves till application is alaive and will be visible only in that aplication. 

we can have multiple spark session for a single spark application. spark session internally creates spark context. context represents a connection to \
spark cluster. context also keeps track of all rdds, cached data as well as configuartions. only one context per application(single JVM).

usual application wont't need to coreate multiple spark sessions. application which needs to support multiple users in parallel in such scenarioes will need multiple session.

** wthere can i see temp view or table crated by df methods.
--> it will be maintain by spark session. spark session offers you catalog. catalog is interface which offres you alter, drop underling database tables and functions.
spark.catalog.listTables.show

globl tmp table can be availbe in global_tmp database.
spark.catalog.listTables("global_tmp").show

** Spark dataFrame Reader gives option called mode to control malformed data. 

There are three values you can specify for it.


1) permissive  -- default. if any of record is mailformed, spark wil put null for all columns and push the entire row into string column _corrupt_record. this allowes you to dum this row to separate location.
2) dropmalformed  -- will drop malformed records.
3) failfast -- will throw exception if any malformed records found.

** Spark dataFrame Writer provide mode option for file based destination.

1)append
2)overwrite
3)erroIfExists  - ignore without any error if file exist.
4)ignore
